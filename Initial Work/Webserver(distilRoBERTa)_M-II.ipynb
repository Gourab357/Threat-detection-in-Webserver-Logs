{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNe52sKr+pYY4IxNar66Y05"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GBNk8-XC3Eyc"},"outputs":[],"source":["import pandas as pd\n","\n","# --- 1. Load all datasets ---\n","logs = pd.read_csv('/content/drive/MyDrive/LLM4Sec/Week3/apache_logs_labelled.csv')\n","ip_entropy = pd.read_csv('/content/drive/MyDrive/LLM4Sec/Week3/ip_entropy_features.csv')\n","geolocation = pd.read_csv('/content/drive/MyDrive/LLM4Sec/Week3/geolocation_enriched.csv')\n","session_stats = pd.read_csv('/content/drive/MyDrive/LLM4Sec/Week3/session_stats.csv')\n","# (Optional) Load time-based and sequence features if you wish to merge them:\n","time_based = pd.read_csv('/content/drive/MyDrive/LLM4Sec/Week3/time_based_features.csv')\n","sequence_patterns = pd.read_csv('/content/drive/MyDrive/LLM4Sec/Week3/sequence_patterns_features.csv')\n","\n","# --- 2. Merge IP-based features ---\n","# Add IP entropy metrics\n","logs = logs.merge(ip_entropy, on='ip', how='left')\n","\n","# Add geolocation features (pick only unique IP info, drop duplicates for safe join)\n","geo_cols = ['ip', 'country_x', 'region', 'city', 'asn', 'reverse_dns']\n","geo_info = geolocation[geo_cols].drop_duplicates('ip')\n","logs = logs.merge(geo_info, on='ip', how='left')\n","\n","# --- 3. Merge Session-level features ---\n","# Take the most recent session per IP (can change to aggregate if you have session_id per row)\n","session_latest = session_stats.sort_values('end_time').groupby('ip').tail(1)\n","session_cols = [col for col in session_stats.columns if col not in ['ip', 'start_time', 'end_time']]\n","logs = logs.merge(session_latest[['ip'] + session_cols], on='ip', how='left')\n","\n","# --- 4. (Optional) Merge Time-based/Sequence features if needed ---\n","# If you want to add time-based, you'll need to round/align timestamps for a good join (not shown here)\n","# logs = logs.merge(time_based, ...)\n","\n","# --- 5. Save unified DataFrame for analysis and modeling ---\n","logs.to_csv('logs_features_unified.csv', index=False)\n","\n","print(\"Unified feature set shape:\", logs.shape)\n","print(logs.head())\n"]},{"cell_type":"code","source":["import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","# Load your merged dataset\n","logs = pd.read_csv('/content/logs_features_unified.csv')\n","\n","# Select only numeric columns for correlation\n","numeric_cols = logs.select_dtypes(include=['number']).columns\n","corr_matrix = logs[numeric_cols].corr()\n","\n","# Plot heatmap\n","plt.figure(figsize=(12, 8))\n","sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n","plt.title('Correlation Heatmap of Numeric Features (Merged Logs)')\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"M4Z6ncNh0t75"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["time_based['datetime'] = pd.to_datetime(time_based['datetime'])\n","time_based.set_index('datetime', inplace=True)\n","\n","time_based['request_count_mean_1h'].plot(figsize=(15, 5), title='Request Rate per Hour - Detecting Spikes')\n","plt.ylabel('Request Count Mean (1h)')\n","plt.show()\n"],"metadata":{"id":"Qqz8GchJ0wZN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logs.columns"],"metadata":{"id":"bCTZNh3o00IR"},"execution_count":null,"outputs":[]}]}